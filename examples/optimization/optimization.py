'''
Стохастическая оптимизация
Обычно методы оптимизации используются для задач, имеющих множество возможных решений, которые зависят от многих переменных, а результат существенно зависит от сочетания этих переменных. У методов оптимизации весьма широкая область применения: в физике они используются для изучения молекулярной динамики, в биологии – для прогнозирования белковых структур, а в информатике – для определения времени работы алгоритма в худшем случае. В НАСА методы оптимизации применяют даже для проектирования антенн с наилучшими эксплуатационными характеристиками.
По существу, оптимизация сводится к поиску наилучшего решения задачи путем апробирования различных решений и сравнения их между собой для оценки качества. Обычно оптимизация применяется в тех случаях, когда число решений слишком велико и перебрать их все невозможно
'''

'''
Планирование путешествия группы людей, которые, отправляясь из разных мест, должны прибыть в одно и то же место, всегда вызывало сложности и представляет собой интересную оптимизационную задачу.
'''

import time
import random
import math

people = [
    ('Seymour','BOS'),
    ('Franny','DAL'),
    ('Zooey','CAK'),
    ('Walt','MIA'),
    ('Buddy','ORD'),
    ('Les','OMA')
]
# Место назначения – аэропорт Ла Гардиа в Нью-Йорке
destination='LGA'



# s = [1, 4, 3, 2, 7, 3, 6, 3, 2, 4, 5, 3]
domain = [(0, 8)] * (len(people) * 2)

'''
Члены семьи живут в разных концах страны и хотят встретиться в Нью-Йорке. Все они должны вылететь в один день и в один день улететь и при этом в целях экономии хотели бы уехать из аэропорта и приехать в него на одной арендованной машине. Ежедневно в Нью-Йорк из мест проживания любого члена семьи отправляются десятки рейсов, все в разное время. Цена билета и время в пути для каждого рейса разные.

Загрузим эти данные в словарь, для которого ключом будет аэропорт отправления и аэропорт назначения, а значением – список, содержащий детальную информацию о рейсах.
'''

flights = {}
# 
for line in open('schedule.txt'):
    origin, dest, depart, arrive, price = line.strip().split(',')
    flights.setdefault((origin, dest), [])

    # Add details to the list of possible flights
    flights[(origin, dest)].append((depart, arrive, int(price)))

'''
Сейчас будет полезно написать служебную функцию getminutes , которая вычисляет смещение данного момента времени от начала суток в минутах. Так нам будет удобнее вычислять время в пути и время ожидания.
'''
def getminutes(t):
    x = time.strptime(t, '%H:%M')
    return x[3] * 60 + x[4]

'''
Для подобных задач необходимо определиться со способом представления потенциальных решений. Каждое число обозначает рейс, которым решил лететь участник группы, причем 0 – это первый рейс в течение суток, 1 – второй и т. д. Так как каждый человек должен лететь туда и обратно, то длина списка в два раза больше количества людей. 
Поскольку интерпретировать решение, имея перед собой лишь список чисел, трудно, нам понадобится функция, которая печатает выбранные рейсы в виде красиво отформатированной таблицы.
'''
def printschedule(r):
    rng = range(int(len(r) / 2))
    for d in rng:
        name = people[d][0]
        origin = people[d][1]
        out = flights[(origin, destination)][int(r[d])]
        ret = flights[(destination, origin)][int(r[d + 1])]
        print(
            '%10s%10s %5s-%5s $%3s %5s-%5s $%3s' % 
            (
                name, origin,
                out[0], out[1], out[2],
                ret[0], ret[1], ret[2]
            )
        )


# printschedule(s)


'''
Целевая функция
Ключом к решению любой задачи оптимизации является целевая функция, и именно ее обычно труднее всего отыскать. Цель оптимизационного алгоритма состоит в том, чтобы найти такой набор входных переменных – в данном случае рейсов, – который минимизирует целевую функцию. Поэтому целевая функция должна возвращать значение, показывающее, насколько данное решение неудовлетворительно. Не существует никакого определенного масштаба неудовлетворительности; требуется лишь, чтобы возвращаемое значение было тем больше, чем хуже решение.

Часто, когда переменных много, бывает трудно сформулировать критерий, хорошее получилось решение или плохое. Рассмотрим несколько параметров, которые можно измерить в примере с групповым путешествием:
    Цена - Полная стоимость всех билетов или, возможно, среднее значение, взвешенное с учетом финансовых возможностей.
    Время в пути - Суммарное время, проведенное всеми членами семьи в полете.
    Время ожидания - Время, проведенное в аэропорту в ожидании прибытия остальных членов группы.
    Время вылета - Если самолет вылетает рано утром, это может увеличивать общую стоимость из-за того, что путешественники не выспятся.
    Время аренды автомобилей - Если группа арендует машину, то вернуть ее следует до того часа, когда она была арендована, иначе придется платить за лишний день.

Определившись с тем, какие переменные влияют на стоимость, нужно решить, как из них составить одно число. В нашем случае можно, например, выразить в деньгах время в пути или время ожидания в аэропорту. Скажем, каждая минута в воздухе эквивалентна $1 (иначе говоря, можно потратить лишние $90 на прямой рейс, экономящий полтора часа), а каждая минута ожидания в аэропорту эквивалентна $0,50. Можно было бы также приплюсовать стоимость лишнего дня аренды машины, если для всех имеет смысл вернуться в аэропорт к более позднему часу. Кроме того, добавляется штраф $50, если машина возвращена в более поздний час, чем арендована.
'''

def schedulecost(sol):
    totalprice = 0
    latestarrival = 0
    earliestdep = 24 * 60

    for d in range(int(len(sol) / 2)):
        # Get the inbound and outbound flights
        origin = people[d][1]
        outbound = flights[(origin, destination)][int(sol[d])]
        returnf = flights[(destination, origin)][int(sol[d + 1])]

        # Total price is the price of all outbound and return flights
        totalprice += outbound[2]
        totalprice += returnf[2]

        # Track the latest arrival and earliest departure
        if latestarrival < getminutes(outbound[1]): 
            latestarrival = getminutes(outbound[1])
        if earliestdep > getminutes(returnf[0]): 
            earliestdep = getminutes(returnf[0])

    # Every person must wait at the airport until the latest person arrives.
    # They also must arrive at the same time and wait for their flights.
    totalwait = 0  
    for d in range(int(len(sol) / 2)):
        origin = people[d][1]
        outbound = flights[(origin, destination)][int(sol[d])]
        returnf = flights[(destination, origin)][int(sol[d + 1])]
        totalwait += latestarrival - getminutes(outbound[1])
        totalwait += getminutes(returnf[0]) - earliestdep  

    # Does this solution require an extra day of car rental? That'll be $50!
    if latestarrival > earliestdep: 
        totalprice += 50

    return totalprice + totalwait


# cost = schedulecost(s)
# print('cost', cost)

'''
Случайный поиск – не самый лучший метод оптимизации, но он позволит нам ясно понять, чего пытаются достичь все алгоритмы, а также послужит эталоном, с которым можно будет сравнивать другие алгоритмы. 
Domain – это список пар, определяющих минимальное и максимальное значения каждой переменной.
costf – это целевая функция;
'''

def randomoptimize(domain,costf):
    best = 999999999
    bestr = None
    for i in range(0, 1000):
        # Create a random solution
        r = [
            float(random.randint(domain[i][0], domain[i][1])) 
            for i in range(len(domain))
        ]

        # Get the cost
        cost = costf(r)

        # Compare it to the best one so far
        if cost < best:
            best = cost
            bestr = r

    return r


'''

s = randomoptimize(domain, schedulecost)
schedulecost(s)
printschedule(s)

   Seymour       BOS  8:04-10:11 $ 95  9:58-11:18 $130
    Franny       DAL  9:08-12:12 $364 10:51-14:16 $256
     Zooey       CAK 10:53-13:36 $189 16:33-18:15 $253
      Walt       MIA 17:07-20:04 $291  9:25-12:46 $295
     Buddy       ORD  9:42-11:32 $169  7:50-10:08 $164
       Les       OMA  7:39-10:24 $219  8:04-10:59 $136

'''



'''
Алгоритм спуска с горы
Альтернативный метод случайного поиска называется алгоритмом спуска с горы (hill climbing). Он начинает со случайного решения и ищет лучшие решения (с меньшим значением целевой функции) по соседству. Можно провести аналогию со спуском с горы, отсюда и название.

В данном случае это означает просмотр таких расписаний, для которых один человек выбирает рейс, вылетающий чуть раньше или чуть позже. Для каждого из соседних расписаний вычисляется стоимость, и расписание с наименьшей стоимостью становится новым решением. Этот процесс повторяется и завершается, когда ни одно из соседних расписаний не дает улучшения стоимости.

Для выбора начального решения эта функция генерирует случайный список чисел из заданного диапазона. Соседи текущего решения ищутся путем посещения каждого элемента списка и создания двух новых списков, в одном из которых этот элемент увеличен на единицу, а в другом уменьшен на единицу. Лучшее из соседних решений становится новым решением. Спускаясь по склону, мы необязательно отыщем наилучшее возможное решение. Найденное решение будет локальным минимумом, то есть лучшим из всех в ближайшей окрестности, но это не означает, что оно вообще лучшее. Решение, лучшее среди всех возможных, называется глобальным минимумом, и именно его хотят найти все алгоритмы оптимизации. Один из возможных подходов к решению проблемы называется спуском с горы со случайным перезапуском.
'''

def hillclimb(domain, costf):
    # Create a random solution
    sol = [
        random.randint(domain[i][0], domain[i][1])
        for i in range(len(domain))
    ]
    # Main loop
    while 1:
        # Create list of neighboring solutions
        neighbors = []

        for j in range(len(domain)):
            # One away in each direction
            if sol[j] > domain[j][0]:
                neighbors.append(sol[0: j] + [sol[j] + 1] + sol[j + 1:])
            if sol[j] < domain[j][1]:
                neighbors.append(sol[0: j] + [sol[j] - 1] + sol[j + 1:])

        # See what the best solution amongst the neighbors is
        current = costf(sol)
        best = current
        for j in range(len(neighbors)):
            cost = costf(neighbors[j])
            if cost < best:
                best = cost
                sol = neighbors[j]

        # If there's no improvement, then we've reached the top
        if best == current:
            break

    return sol


'''
s = hillclimb(domain, schedulecost)
schedulecost(s)
printschedule(s)

Seymour       BOS 18:34-19:36 $136 13:39-15:30 $ 74
    Franny       DAL 13:54-18:02 $294 14:20-17:32 $332
     Zooey       CAK 13:40-15:38 $137 13:37-15:33 $142
      Walt       MIA 14:01-17:24 $338 18:07-21:30 $355
     Buddy       ORD 18:48-21:45 $246 12:08-14:47 $231
       Les       OMA 12:18-14:56 $172 12:31-14:02 $234
'''


'''
Алгоритм имитации отжига
Отжигом называется процесс нагрева образца с последующим медленным охлаждением. Поскольку сначала атомы заставляют попрыгать, а затем постепенно «отпускают вожжи», то они переходят в новую низкоэнергетичную конфигурацию.
Алгоритмическая версия отжига начинается с выбора случайного решения задачи. В ней используется переменная, интерпретируемая как температура, начальное значение которой очень велико, но постепенно уменьшается. На каждой итерации случайным образом выбирается один из параметров решения и изменяется в определенном направлении. Иногда необходимо перейти к худшему решению, чтобы найти лучшее. Алгоритм имитации отжига работает, потому что всегда готов перейти к лучшему решению, но ближе к началу процесса соглашается принять и худшее. По мере того как процесс развивается, алгоритм соглашается на худшее решение все с меньшей охотой, а в конце работы принимает только лучшее.
Для выполнения отжига эта функция сначала генерирует случайное решение в виде вектора нужной длины, все элементы которого попадают в диапазон, определенный параметром domain . Параметры temperature и cool (скорость охлаждения) необязательны. На каждой итерации в качестве i случайно выбирается одна из переменных решения, а dir случайно выбирается между –step и step . Вычисляется стоимость текущего решения и стоимость решения, получающегося изменением переменной i на величину step. В pow(math.e, (-eb - ea) / T), вычисляется вероятность, которая уменьшается с уменьшением T . Если величина, случайно выбранная между 0 и 1, оказывается меньше вычисленной вероятности или если новое решение лучше текущего, то новое решение становится текущим. Цикл продолжается, пока температура почти не достигнет нуля, причем каждый раз температура умножается на скорость охлаждения.
'''

def annealingoptimize(domain, costf, T = 10000.0, cool=0.95, step=1):
    # Initialize the values randomly
    vec = [
        float(random.randint(domain[i][0], domain[i][1])) 
        for i in range(len(domain))
    ]

    while T > 0.1:
        # Choose one of the indices
        i = random.randint(0, len(domain) - 1)

        # Choose a direction to change it
        dir = random.randint(-step, step)

        # Create a new list with one of the values changed
        vecb = vec[:]
        vecb[i] += dir
        if vecb[i] < domain[i][0]: 
            vecb[i] = domain[i][0]
        elif vecb[i] > domain[i][1]: 
            vecb[i] = domain[i][1]

        # Calculate the current cost and the new cost
        ea = costf(vec)
        eb = costf(vecb)
        p = pow(math.e, (-eb - ea) / T)

        # Is it better, or does it make the probability
        # cutoff?
        if (eb < ea or random.random() < p):
            vec = vecb

        # Decrease the temperature
        T = T * cool

    return vec

'''
s = annealingoptimize(domain, schedulecost)
schedulecost(s) 
printschedule(s)

Seymour       BOS 17:11-18:30 $108 13:39-15:30 $ 74
    Franny       DAL 13:54-18:02 $294  7:57-11:15 $347
     Zooey       CAK  8:27-10:45 $139 13:37-15:33 $142
      Walt       MIA 14:01-17:24 $338 15:23-18:49 $150
     Buddy       ORD 15:58-18:40 $173  9:11-10:42 $172
       Les       OMA  9:15-12:03 $ 99  8:04-10:59 $136
'''


'''
Генетические алгоритмы
Еще один класс методов оптимизации, также навеянный природой, называется генетическими алгоритмами. Принцип их работы состоит в том, чтобы создать набор случайных решений, который называется популяцией. На каждом шаге оптимизации целевая функция вычисляется для всей популяции, в результате чего получается ранжированный список решений.
Проранжировав решения, мы создаем новую популяцию, которая называется следующим поколением. Сначала в новую популяцию включаются самые лучшие решения из текущей. Этот процесс называется элитизмом. Кроме них, в следующую популяцию входят совершенно новые решения, получающиеся путем модификации наилучших. Модифицировать решения можно двумя способами. Более простой называется мутацией; обычно это небольшое, простое, случайное изменение существующего решения.
Модифицировать решения можно двумя способами. Более простой называется мутацией; обычно это небольшое, простое, случайное изменение существующего решения.
Другой способ называется скрещиванием (или кроссовером). Состоит он в том, что мы берем какие-нибудь два из лучших решений и как-то комбинируем их.
Размер новой популяции обычно совпадает с размером предыдущей, а создается она путем случайных мутаций и скрещиваний лучших решений.

popsize - Размер популяции.
mutprob - Вероятность того, что новая особь будет результатом мутации, а не скрещивания.
elite - Доля особей в популяции, считающихся хорошими решениями и переходящих в следующее поколение.
maxiter - Количество поколений.

'''

def geneticoptimize(domain, costf, popsize = 50, step = 1, mutprob = 0.2, elite = 0.2, maxiter = 100):
    # Mutation Operation
    # В нашем случае для мутации достаточно выбрать одну из переменных решения и уменьшить либо увеличить ее.
    def mutate(vec):
        i = random.randint(0, len(domain) - 1)
        if random.random() < 0.5 and vec[i] > domain[i][0]:
            return vec[0 : i] + [vec[i] - step] + vec[i + 1:] 
        elif vec[i] < domain[i][1]:
            return vec[0 : i] + [vec[i] + step] + vec[i + 1:]

    # Crossover Operation
    # В нашем примере достаточно взять случайное число элементов из одного решения, а остальные – из другого
    def crossover(r1, r2):
        i = random.randint(1, len(domain) - 2)
        return r1[0:i] + r2[i:]

    # Build the initial population
    pop = []
    for i in range(popsize):
        vec = [
            random.randint(domain[i][0], domain[i][1]) 
            for i in range(len(domain))
        ]
        pop.append(vec)

    # How many winners from each generation?
    topelite = int(elite * popsize)

    # Main loop 
    for i in range(maxiter):
        scores = [(costf(v), v) for v in pop]
        scores.sort()
        ranked = [v for (s, v) in scores]

        # Start with the pure winners
        pop = ranked[0 : topelite]

        # Add mutated and bred forms of the winners
        while len(pop) < popsize:
            if random.random() < mutprob:
                # Mutation
                c = random.randint(0, topelite)
                pop.append(mutate(ranked[c]))
            else:
                # Crossover
                c1 = random.randint(0, topelite)
                c2 = random.randint(0, topelite)
                pop.append(crossover(ranked[c1], ranked[c2]))

        # Print current best score
        print(scores[0][0])

    return scores[0][1]


'''
   Seymour       BOS 12:34-15:02 $109 10:33-12:03 $ 74
    Franny       DAL 10:30-14:57 $290 10:51-14:16 $256
     Zooey       CAK 10:53-13:36 $189 10:32-13:16 $139
      Walt       MIA 11:28-14:40 $248 12:37-15:05 $170
     Buddy       ORD 12:44-14:17 $134 10:33-13:11 $132
       Les       OMA 11:08-13:07 $175 15:07-17:21 $129
'''
s = geneticoptimize(domain, schedulecost)
printschedule(s)